{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i78hRF3ufP5z"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 1: Download EMNIST from Kaggle\n",
        "# ============================================\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "print(\"üì• Configuration Kaggle API...\")\n",
        "print(\"‚ö†Ô∏è IMPORTANT: Download your kaggle.json from https://www.kaggle.com/settings/account\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# For Google Colab: Upload kaggle.json\n",
        "try:\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"\\nüìÇ Please upload your kaggle.json file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Configure Kaggle\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    print(\"‚úÖ Kaggle configured successfully\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Not running in Colab. Please ensure kaggle.json is in ~/.kaggle/\")\n",
        "    print(\"   Download from: https://www.kaggle.com/settings/account\")\n",
        "\n",
        "# Download EMNIST dataset\n",
        "print(\"\\nüìä Downloading EMNIST dataset (~560MB, may take 2-5 minutes)...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "!pip install -q kaggle\n",
        "!kaggle datasets download -d crawford/emnist\n",
        "\n",
        "print(\"\\nüì¶ Extracting dataset...\")\n",
        "!unzip -q emnist.zip -d emnist_data\n",
        "\n",
        "print(\"‚úÖ Dataset downloaded and extracted to 'emnist_data/' folder\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqr8KLD_drDM"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 2: Load EMNIST Data\n",
        "# ============================================\n",
        "DATASET_SPLIT = 'digits'  # Options: 'digits', 'letters', 'balanced', 'byclass', 'bymerge', 'mnist'\n",
        "\n",
        "# Load training data\n",
        "train_data = pd.read_csv(f'emnist_data/emnist-{DATASET_SPLIT}-train.csv', header=None)\n",
        "test_data = pd.read_csv(f'emnist_data/emnist-{DATASET_SPLIT}-test.csv', header=None)\n",
        "\n",
        "# Separate features and labels\n",
        "y_train = train_data.iloc[:, 0].values\n",
        "X_train = train_data.iloc[:, 1:].values\n",
        "\n",
        "y_test = test_data.iloc[:, 0].values\n",
        "X_test = test_data.iloc[:, 1:].values\n",
        "\n",
        "# EMNIST images are 28x28, reshape them\n",
        "img_size = 28\n",
        "X_train = X_train.reshape(-1, img_size, img_size, 1)\n",
        "X_test = X_test.reshape(-1, img_size, img_size, 1)\n",
        "\n",
        "# Rotate -90¬∞ and mirror images (EMNIST specific correction)\n",
        "X_train = np.rot90(X_train, k=3, axes=(1, 2))\n",
        "X_train = np.flip(X_train, axis=2)\n",
        "\n",
        "X_test = np.rot90(X_test, k=3, axes=(1, 2))\n",
        "X_test = np.flip(X_test, axis=2)\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Image shape: {X_train.shape[1:]}\")\n",
        "print(f\"Dataset split: {DATASET_SPLIT}\")\n",
        "\n",
        "# Load class mapping if balanced dataset\n",
        "CLASS_MAPPING = None\n",
        "if DATASET_SPLIT == 'balanced':\n",
        "    try:\n",
        "        mapping_file = 'emnist_data/emnist-balanced-mapping.txt'\n",
        "        class_mapping = {}\n",
        "\n",
        "        with open(mapping_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 2:\n",
        "                    class_idx = int(parts[0])\n",
        "                    ascii_code = int(parts[1])\n",
        "                    character = chr(ascii_code)\n",
        "                    class_mapping[class_idx] = character\n",
        "\n",
        "        CLASS_MAPPING = class_mapping\n",
        "        print(f\"\\n‚úÖ Loaded class mapping: {len(class_mapping)} classes\")\n",
        "        print(f\"üì§ All Characters: {' '.join([class_mapping[i] for i in sorted(class_mapping.keys())])}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ö†Ô∏è Mapping file not found\")\n",
        "        CLASS_MAPPING = {i: str(i) for i in range(num_classes)}\n",
        "\n",
        "# Visualize samples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X_train[i].squeeze(), cmap='gray')\n",
        "    if CLASS_MAPPING:\n",
        "        label_char = CLASS_MAPPING.get(y_train[i], str(y_train[i]))\n",
        "        ax.set_title(f\"Label: {y_train[i]} ({label_char})\")\n",
        "    else:\n",
        "        ax.set_title(f\"Label: {y_train[i]}\")\n",
        "    ax.axis('off')\n",
        "plt.suptitle(f'EMNIST {DATASET_SPLIT.upper()} - Sample Images', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cFHwxZduex"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 3: Build TinyML Model\n",
        "# ============================================\n",
        "def build_tiny_model(input_shape=(28, 28, 1), num_classes=10):\n",
        "    \"\"\"\n",
        "    Lightweight CNN optimized for TinyML deployment\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Block 1\n",
        "    x = layers.Conv2D(16, 3, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = layers.Conv2D(32, 3, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    # Classifier\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model = build_tiny_model(input_shape=(28, 28, 1), num_classes=num_classes)\n",
        "model.summary()\n",
        "\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"   Total parameters: {model.count_params():,}\")\n",
        "print(f\"   Target classes: {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MgBRcU1dv8J"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 4: Train Model\n",
        "# ============================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üöÄ TRAINING MODEL\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Balanced data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomRotation(0.10),\n",
        "    layers.RandomZoom(0.10),\n",
        "    layers.RandomTranslation(0.10, 0.10),\n",
        "    layers.RandomContrast(0.1),\n",
        "])\n",
        "\n",
        "def augment_data(images, labels):\n",
        "    images = data_augmentation(images, training=True)\n",
        "    return images, labels\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(10000).batch(BATCH_SIZE).map(augment_data).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=8,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=4,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_model.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Max epochs: 40\")\n",
        "print(f\"Initial learning rate: 1e-3\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=40,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.plot(history.history['accuracy'], label='train')\n",
        "ax1.plot(history.history['val_accuracy'], label='val')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history.history['loss'], label='train')\n",
        "ax2.plot(history.history['val_loss'], label='val')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIEKo-mldyA-"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 5: Evaluate Model\n",
        "# ============================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä FINAL EVALUATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"‚úÖ Test accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "print(f\"   Test loss: {test_loss:.4f}\")\n",
        "\n",
        "# Calculate per-class accuracy if mapping exists\n",
        "if CLASS_MAPPING:\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "    y_pred = model.predict(X_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    target_names = [CLASS_MAPPING.get(i, str(i)) for i in range(num_classes)]\n",
        "\n",
        "    print(f\"\\nüìã Detailed Classification Report:\")\n",
        "    print(\"-\" * 70)\n",
        "    report = classification_report(y_test, y_pred_classes, target_names=target_names)\n",
        "    print(report)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "\n",
        "    print(f\"\\n‚ö†Ô∏è Bottom 5 Classes by Accuracy:\")\n",
        "    worst_5 = np.argsort(per_class_acc)[:5]\n",
        "    for idx in worst_5:\n",
        "        char = CLASS_MAPPING.get(idx, str(idx))\n",
        "        acc = per_class_acc[idx]\n",
        "        print(f\"   Class {idx} ({char}): {acc*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruLXe2h9fQ9F"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 6: Convert to TFLite (INT8 Quantized)\n",
        "# ============================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üîß CONVERTING TO TFLITE (INT8)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "def representative_dataset():\n",
        "    for i in range(100):\n",
        "        yield [X_train[i:i+1].astype(np.float32)]\n",
        "\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save TFLite model\n",
        "model_filename = f'emnist_{DATASET_SPLIT}_int8.tflite'\n",
        "with open(model_filename, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"‚úÖ TFLite model saved: {model_filename}\")\n",
        "print(f\"   Model size: {len(tflite_model) / 1024:.2f} KB\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSUWWDP5gKLC"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 7: Test TFLite Model\n",
        "# ============================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üß™ TESTING TFLITE MODEL\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=model_filename)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"TFLite Model Details:\")\n",
        "print(f\"Input shape: {input_details[0]['shape']}\")\n",
        "print(f\"Input type: {input_details[0]['dtype']}\")\n",
        "print(f\"Output shape: {output_details[0]['shape']}\")\n",
        "print(f\"Output type: {output_details[0]['dtype']}\")\n",
        "\n",
        "def run_tflite_inference(interpreter, input_data, input_details, output_details):\n",
        "    \"\"\"Run inference on TFLite model with proper quantization\"\"\"\n",
        "    input_info = input_details[0]\n",
        "    output_info = output_details[0]\n",
        "\n",
        "    # Quantize input\n",
        "    if input_info['dtype'] == np.int8:\n",
        "        scale, zero_point = input_info['quantization']\n",
        "        input_data = np.round(input_data / scale + zero_point)\n",
        "        input_data = np.clip(input_data, -128, 127).astype(np.int8)\n",
        "\n",
        "    interpreter.set_tensor(input_info['index'], input_data)\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output_data = interpreter.get_tensor(output_info['index'])\n",
        "\n",
        "    # Dequantize output for visualization\n",
        "    if output_info['dtype'] == np.int8:\n",
        "        scale, zero_point = output_info['quantization']\n",
        "        output_data = (output_data.astype(np.float32) - zero_point) * scale\n",
        "\n",
        "    return output_data\n",
        "\n",
        "# Test on samples\n",
        "num_test_samples = 10\n",
        "predictions = []\n",
        "\n",
        "for i in range(num_test_samples):\n",
        "    input_data = X_test[i:i+1]\n",
        "    output_data = run_tflite_inference(interpreter, input_data, input_details, output_details)\n",
        "    predicted_class = np.argmax(output_data)\n",
        "    predictions.append(predicted_class)\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X_test[i].squeeze(), cmap='gray')\n",
        "    correct = predictions[i] == y_test[i]\n",
        "    color = 'green' if correct else 'red'\n",
        "\n",
        "    if CLASS_MAPPING:\n",
        "        pred_char = CLASS_MAPPING.get(predictions[i], str(predictions[i]))\n",
        "        true_char = CLASS_MAPPING.get(y_test[i], str(y_test[i]))\n",
        "        ax.set_title(f\"Pred: {pred_char}, True: {true_char}\", color=color)\n",
        "    else:\n",
        "        ax.set_title(f\"Pred: {predictions[i]}, True: {y_test[i]}\", color=color)\n",
        "\n",
        "    ax.axis('off')\n",
        "plt.suptitle('TFLite Model Predictions', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ Accuracy on {num_test_samples} samples: {sum([p == y_test[i] for i, p in enumerate(predictions)]) / num_test_samples * 100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VcCopHagMPs"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 8: Test with Custom Image (Optional)\n",
        "# ============================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üñºÔ∏è TESTING WITH CUSTOM IMAGE\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "try:\n",
        "    img = cv2.imread(\"test.png\", cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if img is not None:\n",
        "        # Preprocess\n",
        "        img_resized = cv2.resize(img, (28, 28))\n",
        "        img_normalized = img_resized / 255.0\n",
        "        input_img = img_normalized.reshape(1, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "        # Run inference\n",
        "        output_data = run_tflite_inference(interpreter, input_img, input_details, output_details)\n",
        "        predicted_class = np.argmax(output_data)\n",
        "\n",
        "        # Display result\n",
        "        plt.imshow(img_normalized, cmap='gray')\n",
        "        if CLASS_MAPPING:\n",
        "            pred_char = CLASS_MAPPING.get(predicted_class, str(predicted_class))\n",
        "            plt.title(f\"Predicted: {pred_char} (class {predicted_class})\")\n",
        "        else:\n",
        "            plt.title(f\"Predicted: {predicted_class}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"‚úÖ Predicted class: {predicted_class}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not load 'test.png'. Skipping custom image test.\")\n",
        "        print(\"   To test with your own image, place a 'test.png' file in the working directory.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error loading custom image: {e}\")\n",
        "    print(\"   Skipping custom image test.\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ PIPELINE COMPLETE\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}